Pattern Name,Use Case,pandas Code,polars Code,Performance
Load CSV File,Read data from CSV file,"df = pd.read_csv('file.csv', parse_dates=['date_col'])","df = pl.read_csv('file.csv')","Use dtype parameter to reduce memory; usecols for subset"
Load Excel File,Read data from Excel file,"df = pd.read_excel('file.xlsx', sheet_name='Sheet1')","df = pl.read_excel('file.xlsx')","Specify sheet_name; engine='openpyxl' for xlsx"
Load Multiple CSVs,Combine CSVs from folder,"df = pd.concat([pd.read_csv(f) for f in glob.glob('*.csv')])","df = pl.concat([pl.read_csv(f) for f in glob.glob('*.csv')])","Use ignore_index=True to reset index"
Database Connection,Connect to SQL database,"from sqlalchemy import create_engine; engine = create_engine('postgresql://...'); df = pd.read_sql(query, engine)","df = pl.read_database(query, connection_uri)","Use connection pooling for multiple queries"
Filter Rows,Select rows matching condition,"df = df[df['status'] == 'active']; df = df[df['value'] > 100]","df = df.filter(pl.col('status') == 'active')","Chain filters or use & for multiple conditions"
Select Columns,Choose specific columns,"df = df[['col1', 'col2', 'col3']]; df = df.drop(columns=['unwanted'])","df = df.select(['col1', 'col2', 'col3'])","Select early to reduce memory"
Rename Columns,Change column names,"df = df.rename(columns={'old': 'new', 'old2': 'new2'})","df = df.rename({'old': 'new'})","Use dict for multiple renames"
Sort Data,Order by column values,"df = df.sort_values(['col1', 'col2'], ascending=[True, False])","df = df.sort(['col1', 'col2'], descending=[False, True])","Sort after filtering for efficiency"
Group By Aggregate,Aggregate data by groups,"df.groupby('category').agg({'value': 'sum', 'count': 'size'})","df.group_by('category').agg([pl.col('value').sum(), pl.len()])","Named aggregations: agg(total=('value', 'sum'))"
Pivot Table,Create pivot table,"df.pivot_table(index='row', columns='col', values='value', aggfunc='sum', fill_value=0)","df.pivot(index='row', columns='col', values='value')","Use margins=True for totals"
Melt Unpivot,Convert wide to long format,"df.melt(id_vars=['id'], value_vars=['col1', 'col2'], var_name='variable', value_name='value')","df.melt(id_vars=['id'])","Inverse of pivot operation"
Join Merge,Combine two dataframes,"df = pd.merge(df1, df2, on='key', how='left')","df = df1.join(df2, on='key', how='left')","Validate: how='left'/'right'/'inner'/'outer'"
Concatenate DataFrames,Stack dataframes vertically,"df = pd.concat([df1, df2], ignore_index=True)","df = pl.concat([df1, df2])","axis=0 for rows; axis=1 for columns"
Apply Function,Transform values with function,"df['new'] = df['col'].apply(lambda x: x * 2)","df = df.with_columns(pl.col('col').map_elements(lambda x: x * 2))","Vectorized operations are faster than apply"
Rolling Window,Calculate rolling statistics,"df['rolling_mean'] = df['value'].rolling(window=7).mean()","df.with_columns(pl.col('value').rolling_mean(7))","Specify min_periods for edge handling"
Date Extraction,Extract date components,"df['year'] = df['date'].dt.year; df['month'] = df['date'].dt.month; df['weekday'] = df['date'].dt.dayofweek","df.with_columns(pl.col('date').dt.year().alias('year'))","dt accessor for date operations"
Date Difference,Calculate days between dates,"df['days'] = (df['end_date'] - df['start_date']).dt.days","df.with_columns((pl.col('end_date') - pl.col('start_date')).dt.total_days())","Result is timedelta; use .days for integer"
Lag/Lead Values,Get previous or next row values,"df['prev_value'] = df.groupby('id')['value'].shift(1)","df.with_columns(pl.col('value').shift(1).over('id'))","shift(-1) for next value (lead)"
Cumulative Sum,Running total,"df['cumsum'] = df.groupby('category')['value'].cumsum()","df.with_columns(pl.col('value').cum_sum().over('category'))","Order matters - sort first if needed"
Rank Values,Rank within groups,"df['rank'] = df.groupby('category')['value'].rank(ascending=False)","df.with_columns(pl.col('value').rank().over('category'))","method='min'/'dense'/'first' for tie handling"
Percent of Total,Calculate percentage of group total,"df['pct'] = df['value'] / df.groupby('category')['value'].transform('sum')","df.with_columns((pl.col('value') / pl.col('value').sum().over('category')).alias('pct'))","transform applies group result back to rows"
Fill Missing Forward,Forward fill nulls,"df['col'] = df['col'].fillna(method='ffill')","df.with_columns(pl.col('col').forward_fill())","bfill for backward fill"
Replace Values,Map values to new values,"df['col'] = df['col'].replace({'old1': 'new1', 'old2': 'new2'})","df.with_columns(pl.col('col').replace({'old1': 'new1'}))","Use map for complex transformations"
Binning Discretize,Convert continuous to categorical,"df['bin'] = pd.cut(df['value'], bins=[0, 10, 50, 100], labels=['low', 'med', 'high'])","df.with_columns(pl.col('value').cut([10, 50, 100]))","qcut for equal-frequency bins"
One Hot Encoding,Convert categorical to dummies,"df = pd.get_dummies(df, columns=['category'], prefix='cat')","df.to_dummies(columns=['category'])","drop_first=True to avoid multicollinearity"
Value Counts,Count occurrences of each value,"df['col'].value_counts(normalize=True)","df['col'].value_counts()","normalize=True for percentages"
Describe Statistics,Summary statistics,"df.describe(include='all', percentiles=[.25, .5, .75])","df.describe()","include='all' for non-numeric columns"
Correlation Matrix,Calculate correlations,"df.select_dtypes(include='number').corr()","df.select(pl.numeric_columns()).pearson_corr()","Use method='spearman' for non-linear"
Cross Tabulation,Frequency table for two columns,"pd.crosstab(df['col1'], df['col2'], normalize='index')","N/A - use group_by and pivot","normalize='all'/'index'/'columns'"
Sample Data,Random sample of rows,"df.sample(n=1000) or df.sample(frac=0.1)","df.sample(n=1000)","random_state for reproducibility"
